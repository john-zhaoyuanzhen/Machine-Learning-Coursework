{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist #to import our dataset\n",
    "from tensorflow.keras.models import Sequential, Model # imports our type of network\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout # imports our layers we want to use\n",
    "\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy #loss function\n",
    "from tensorflow.keras.optimizers import Adam, SGD #optimisers\n",
    "from tensorflow.keras.utils import to_categorical #some function for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 19\n",
    "epochs = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.concatenate((x_train, x_test), axis=0)\n",
    "labels = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "np.random.seed(73289)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_dataset(iamges, labels, dataset_size=10000):\n",
    "    combined_images = []\n",
    "    combined_labels = []\n",
    "    \n",
    "    for _ in range(dataset_size):\n",
    "        idx1, idx2 = np.random.choice(np.arange(len(images)), 2, replace=True)\n",
    "        img1, img2 = images[idx1], images[idx2]\n",
    "        label1, label2 = labels[idx1], labels[idx2]\n",
    "        \n",
    "        combined_image = np.vstack((img1, img2))\n",
    "        combined_label = label1 + label2\n",
    "        \n",
    "        combined_images.append(combined_image)\n",
    "        combined_labels.append(combined_label)\n",
    "        \n",
    "    combined_images = np.array(combined_images).reshape(-1, 56, 28, 1)\n",
    "    combined_labels = np.array(combined_labels)\n",
    "    \n",
    "    return combined_images, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  70000\n",
      "Validation size:  10000\n",
      "Test size:  20000\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 100000\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "print(\"Train size: \", train_size)\n",
    "print(\"Validation size: \", val_size)\n",
    "print(\"Test size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_images, combined_labels = create_combined_dataset(images, labels, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 28, 1)\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21015ac8f20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(combined_images[0].shape)\n",
    "print(combined_labels[0])\n",
    "plt.imshow(combined_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_images, train_labels = combined_images[:train_size], combined_labels[:train_size]\n",
    "val_images, val_labels = combined_images[train_size:train_size+val_size], combined_labels[train_size:train_size+val_size]\n",
    "test_images, test_labels = combined_images[train_size+val_size:], combined_labels[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "val_labels = to_categorical(val_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape:  (70000, 56, 28, 1)\n",
      "Validation images shape:  (10000, 56, 28, 1)\n",
      "Test images shape:  (20000, 56, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train images shape: \", train_images.shape)\n",
    "print(\"Validation images shape: \", val_images.shape)\n",
    "print(\"Test images shape: \", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(56, 28, 1)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3241 - loss: 0.0409 - val_accuracy: 0.7362 - val_loss: 0.0197\n",
      "Epoch 2/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7449 - loss: 0.0191 - val_accuracy: 0.8124 - val_loss: 0.0141\n",
      "Epoch 3/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8208 - loss: 0.0138 - val_accuracy: 0.8426 - val_loss: 0.0121\n",
      "Epoch 4/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8536 - loss: 0.0115 - val_accuracy: 0.8642 - val_loss: 0.0105\n",
      "Epoch 5/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8733 - loss: 0.0100 - val_accuracy: 0.8734 - val_loss: 0.0099\n",
      "Epoch 6/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8876 - loss: 0.0088 - val_accuracy: 0.8872 - val_loss: 0.0087\n",
      "Epoch 7/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9021 - loss: 0.0077 - val_accuracy: 0.8990 - val_loss: 0.0080\n",
      "Epoch 8/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9115 - loss: 0.0071 - val_accuracy: 0.9051 - val_loss: 0.0075\n",
      "Epoch 9/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9215 - loss: 0.0063 - val_accuracy: 0.9081 - val_loss: 0.0074\n",
      "Epoch 10/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9282 - loss: 0.0059 - val_accuracy: 0.9144 - val_loss: 0.0070\n",
      "Epoch 11/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9309 - loss: 0.0056 - val_accuracy: 0.9217 - val_loss: 0.0064\n",
      "Epoch 12/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9373 - loss: 0.0051 - val_accuracy: 0.9238 - val_loss: 0.0062\n",
      "Epoch 13/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9405 - loss: 0.0049 - val_accuracy: 0.9243 - val_loss: 0.0061\n",
      "Epoch 14/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9410 - loss: 0.0049 - val_accuracy: 0.9267 - val_loss: 0.0058\n",
      "Epoch 15/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9440 - loss: 0.0045 - val_accuracy: 0.9289 - val_loss: 0.0057\n",
      "Epoch 16/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9490 - loss: 0.0042 - val_accuracy: 0.9329 - val_loss: 0.0053\n",
      "Epoch 17/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9524 - loss: 0.0040 - val_accuracy: 0.9312 - val_loss: 0.0055\n",
      "Epoch 18/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9513 - loss: 0.0040 - val_accuracy: 0.9316 - val_loss: 0.0055\n",
      "Epoch 19/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9530 - loss: 0.0039 - val_accuracy: 0.9348 - val_loss: 0.0054\n",
      "Epoch 20/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9550 - loss: 0.0037 - val_accuracy: 0.9361 - val_loss: 0.0053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21102f80350>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=20,\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - accuracy: 0.9298 - loss: 0.0058\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mae = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    num_units_layer1 = trial.suggest_int(\"units_layer1\", 256, 512, step=32)\n",
    "    num_units_layer2 = trial.suggest_int(\"units_layer2\", 256, 512, step=32)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential([\n",
    "        Input(shape=(56, 28, 1)),\n",
    "        Flatten(),\n",
    "        Dense(num_units_layer1, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_units_layer2, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_images, train_labels,\n",
    "        validation_data=(val_images, val_labels),\n",
    "        epochs=10,  # Use a small number of epochs for tuning\n",
    "        batch_size=batch_size,\n",
    "        verbose=0  # Suppress output for faster tuning\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on validation data\n",
    "    val_accuracy = history.history['val_accuracy'][-1]  # Last epoch's validation accuracy\n",
    "    return -val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 01:27:10,532] A new study created in memory with name: no-name-e14b92d1-8b91-44f0-a066-90b3e0e26a3d\n",
      "[I 2024-11-29 01:27:21,405] Trial 0 finished with value: -0.9114999771118164 and parameters: {'units_layer1': 320, 'units_layer2': 512, 'dropout_rate': 0.4, 'learning_rate': 0.0010601102234192555, 'batch_size': 256}. Best is trial 0 with value: -0.9114999771118164.\n",
      "[I 2024-11-29 01:27:29,320] Trial 1 finished with value: -0.8845000267028809 and parameters: {'units_layer1': 256, 'units_layer2': 288, 'dropout_rate': 0.4, 'learning_rate': 0.0012649552802624507, 'batch_size': 512}. Best is trial 0 with value: -0.9114999771118164.\n",
      "[I 2024-11-29 01:27:49,732] Trial 2 finished with value: -0.9286999702453613 and parameters: {'units_layer1': 320, 'units_layer2': 448, 'dropout_rate': 0.2, 'learning_rate': 0.0010826915880664443, 'batch_size': 128}. Best is trial 2 with value: -0.9286999702453613.\n",
      "[I 2024-11-29 01:28:09,253] Trial 3 finished with value: -0.9028000235557556 and parameters: {'units_layer1': 288, 'units_layer2': 448, 'dropout_rate': 0.5, 'learning_rate': 0.0019700913645193893, 'batch_size': 128}. Best is trial 2 with value: -0.9286999702453613.\n",
      "[I 2024-11-29 01:28:29,727] Trial 4 finished with value: -0.39010000228881836 and parameters: {'units_layer1': 320, 'units_layer2': 448, 'dropout_rate': 0.5, 'learning_rate': 0.007219239876808277, 'batch_size': 128}. Best is trial 2 with value: -0.9286999702453613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'units_layer1': 320, 'units_layer2': 448, 'dropout_rate': 0.2, 'learning_rate': 0.0010826915880664443, 'batch_size': 128}\n",
      "Best validation accuracy: 0.9287\n"
     ]
    }
   ],
   "source": [
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)  # Run 50 trials\n",
    "\n",
    "best_params = study.best_params\n",
    "best_accuracy = -study.best_value  # Negate the value to get accuracy\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3792 - loss: 1.8874 - val_accuracy: 0.7807 - val_loss: 0.7199\n",
      "Epoch 2/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7916 - loss: 0.6867 - val_accuracy: 0.8408 - val_loss: 0.5187\n",
      "Epoch 3/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8568 - loss: 0.4727 - val_accuracy: 0.8775 - val_loss: 0.4021\n",
      "Epoch 4/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8884 - loss: 0.3591 - val_accuracy: 0.8940 - val_loss: 0.3499\n",
      "Epoch 5/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9102 - loss: 0.2914 - val_accuracy: 0.9039 - val_loss: 0.3160\n",
      "Epoch 6/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9265 - loss: 0.2365 - val_accuracy: 0.9112 - val_loss: 0.2936\n",
      "Epoch 7/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9366 - loss: 0.2018 - val_accuracy: 0.9217 - val_loss: 0.2700\n",
      "Epoch 8/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9428 - loss: 0.1769 - val_accuracy: 0.9240 - val_loss: 0.2649\n",
      "Epoch 9/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9510 - loss: 0.1502 - val_accuracy: 0.9299 - val_loss: 0.2496\n",
      "Epoch 10/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9555 - loss: 0.1397 - val_accuracy: 0.9307 - val_loss: 0.2537\n",
      "Epoch 11/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9589 - loss: 0.1234 - val_accuracy: 0.9331 - val_loss: 0.2528\n",
      "Epoch 12/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9603 - loss: 0.1188 - val_accuracy: 0.9345 - val_loss: 0.2422\n",
      "Epoch 13/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9635 - loss: 0.1091 - val_accuracy: 0.9328 - val_loss: 0.2490\n",
      "Epoch 14/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.0996 - val_accuracy: 0.9371 - val_loss: 0.2455\n",
      "Epoch 15/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0970 - val_accuracy: 0.9353 - val_loss: 0.2450\n",
      "Epoch 16/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9704 - loss: 0.0898 - val_accuracy: 0.9396 - val_loss: 0.2466\n",
      "Epoch 17/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.0819 - val_accuracy: 0.9390 - val_loss: 0.2415\n",
      "Epoch 18/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.0802 - val_accuracy: 0.9407 - val_loss: 0.2398\n",
      "Epoch 19/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9751 - loss: 0.0781 - val_accuracy: 0.9402 - val_loss: 0.2469\n",
      "Epoch 20/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.0758 - val_accuracy: 0.9407 - val_loss: 0.2535\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step - accuracy: 0.9331 - loss: 0.2963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30350470542907715, 0.9323999881744385]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the best model with the optimized parameters\n",
    "best_model = Sequential([\n",
    "    Flatten(input_shape=(56, 28, 1)),\n",
    "    Dense(best_params['units_layer1'], activation='relu'),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "    Dense(best_params['units_layer2'], activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "best_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_model.fit(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=20,\n",
    "    batch_size=best_params['batch_size']\n",
    ")\n",
    "\n",
    "best_model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
